{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMMH98GdzyiFaSgQGAAaobB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/buistung/Advanced-Virtual-Assistant/blob/main/advanced_virtual_assistant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_BLAS=ON -DLLAMA_BLAS_VENDOR=OpenBLAS\" pip install llama-cpp-python"
      ],
      "metadata": {
        "collapsed": true,
        "id": "xWlV80bWhyuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install huggingface-hub\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7PbUmY51hyxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli download TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF \\\n",
        "tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf \\\n",
        "--local-dir . \\\n",
        "--local-dir-use-symlinks False"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ZaeI1Fx9hy0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers"
      ],
      "metadata": {
        "collapsed": true,
        "id": "SyvQKkTEtQ9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes accelerate"
      ],
      "metadata": {
        "id": "vIgqu1kNuaeC",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "class Assistant():\n",
        "    def __init__(self, model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" ):\n",
        "        self.pipe = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "    def format_messages(self, user_input):\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"\",\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": user_input},\n",
        "        ]\n",
        "        return self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "\n",
        "    def generate_response(self, user_input, max_new_tokens=128, temperature=0.5, top_k=1):\n",
        "        prompt = self.format_messages(user_input)\n",
        "        outputs = self.pipe(prompt, max_new_tokens=max_new_tokens, do_sample=True, temperature=temperature, top_k=top_k,)\n",
        "        return outputs[0][\"generated_text\"]\n",
        "\n",
        "    def extract_assistant_response(self, text):\n",
        "        start_tag = \"<|assistant|>\"\n",
        "        start_index = text.find(start_tag)\n",
        "        if start_index == -1:\n",
        "            return \"No assistant response found\"\n",
        "        extracted_text = text[start_index + len(start_tag):].strip()\n",
        "        return extracted_text\n",
        "\n",
        "    def chat(self):\n",
        "        print(\"I am a virtual assitant, type 'exit' to quit the program\")\n",
        "        print(\"---------------\")\n",
        "        print('Assistant: Hello, how can i help you?')\n",
        "        print(\"---------------\")\n",
        "        while True:\n",
        "            user_input = input('User: ')\n",
        "            if user_input.lower() == 'exit':\n",
        "                print('Good bye, have a great day!')\n",
        "                break\n",
        "            else:\n",
        "                response = self.generate_response(user_input)\n",
        "                extracted_text = self.extract_assistant_response(response)\n",
        "                print(\"---------------\")\n",
        "                print(f\"Assistant: {extracted_text}\")\n",
        "                print(\"---------------\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    assistant = Assistant()\n",
        "    assistant.chat()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "rY1PT-k2iLZM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}